








# Standart libs
import os
import re
import string
import zipfile
import tempfile
# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
# Data
import pandas as pd
import numpy as np
# Sklearn for ML
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.metrics.pairwise import cosine_similarity
# from sklearn.decomposition import LatentDirichletAllocation
# Transformers
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
# PyTorch
import torch.optim as optim
import torch
from torch.utils.data import Dataset
from torch.utils.data import Subset
# Web-parsing
from bs4 import BeautifulSoup
import requests
# Gensim for topic modeling
#from gensim.models.phrases import Phrases, Phraser
from gensim.corpora import Dictionary
from gensim.models import TfidfModel, LdaModel, CoherenceModel
from gensim import corpora#, models, similarities


# Обработка текста 
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import string
import spacy


nltk.download("punkt")
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download('punkt_tab')
spacy.cli.download("en_core_web_sm")








zip_path = 'bbc.zip'
data_path = 'bbc'


# Разархивируем bbc.zip
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall()


# Название классов новостных статей
lebels = os.listdir(data_path)


lebels





def load_data(path):
    """Loads text and labels from categorized directories.
    Params:
        path: str
            Path to original data
    Returns:
        pd.DataFrame: 
            Dataset with text and labels
    """
    texts = []
    labels = []
    for category in os.listdir(path):
        category_path = os.path.join(path, category)
        if os.path.isdir(category_path):
            for filename in os.listdir(category_path):
                file_path = os.path.join(category_path, filename)
                with open(file_path, 'r', encoding='latin-1') as file:
                    texts.append(file.read())
                    labels.append(category)
    return pd.DataFrame({'text': texts, 'label': labels})


# Загрузка данных
df = load_data(data_path)


df.info()


Видим что у нас 2221 новостей и нет нулевых данных.


df.head()








# Пригодится для токенизации
nlp = spacy.load("en_core_web_sm")


def preprocess_text(text):
    """
    Cleans the text by removing punctuation, stop words, 
    and performing lemmatization, converts text to lowercase.

    Parameters:
    text (str): The input text to be processed.

    Returns:
    str: The normalized text.
    """
    # Убираем мусорные символы кодировки
    text = text.encode("latin1").decode("utf-8", "ignore")

    # Обрабатываем текст 
    doc = nlp(text)
    tokens = []
    
    for token in doc:
        # Убираем стоп-слова, именованые сущности и токены с цифрами
        if not token.is_stop and token.is_alpha and not token.ent_type_: 
            tokens.append(token.lemma_)

    # Собираем текст
    cleaned_text = " ".join(tokens)
    # Удаляем знаки препинания 
    cleaned_text = cleaned_text.translate(str.maketrans("", "", string.punctuation))
    # Убираем лишние пробелы
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()  
    return cleaned_text.lower()


df["clean_text"] = df["text"].apply(lambda x: preprocess_text(x))





df.head()


df.text[0]


df.clean_text[0]





# Визуализация распределения классов
plt.figure(figsize=(8, 5))
ax = sns.barplot(x=df['label'].value_counts().index, y=df['label'].value_counts().values)
plt.title('Class Distribution', fontsize=14)
plt.xlabel('Categories', fontsize=12)
plt.ylabel('Number of Articles', fontsize=12)
plt.xticks(rotation=45)

for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), 
                ha='center', va='bottom', fontsize=12, fontweight='bold', color='black')

plt.show()








class Dataset_Loader(Dataset):
    """
    A custom PyTorch Dataset class to load and preprocess text data from a zip-file.
    
    The dataset is expected to be organized into subfolders by category (e.g., 'sport', 'business', etc.).
    Each subfolder contains text files corresponding to news articles. This class handles extraction,
    reading, and prepares structured samples for model training or evaluation.
    """

    def __init__(self, zip_path):
        """
        Initializes the dataset by extracting the zip archive, discovering categories, 
        preprocessing all text files.

        Args:
            zip_path (str): Path to the zip file containing the dataset.
        """
        self.zip_path = zip_path

        # Temporary directory for extraction
        self.temp_dir = tempfile.TemporaryDirectory()
        self.extracted_path = self.temp_dir.name

        # Extract archive
        with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:
            zip_ref.extractall(self.extracted_path)

        # Try to detect the root directory containing class folders
        top_level_items = os.listdir(self.extracted_path)

        if len(top_level_items) == 1 and os.path.isdir(os.path.join(self.extracted_path, top_level_items[0])):
            # Case: archive has a single root folder (e.g. bbc/)
            self.folder_path = os.path.join(self.extracted_path, top_level_items[0])
        else:
            # Case: archive has class folders directly in root
            self.folder_path = self.extracted_path

        # Discover category folders
        self.categories = sorted([
            d for d in os.listdir(self.folder_path)
            if os.path.isdir(os.path.join(self.folder_path, d))
        ])
        self.label_map = {category: idx for idx, category in enumerate(self.categories)}

        # Collect and preprocess all samples
        self.samples = []
        for category in self.categories:
            folder = os.path.join(self.folder_path, category)
            for filename in os.listdir(folder):
                if filename.endswith(".txt"):
                    filepath = os.path.join(folder, filename)
                    with open(filepath, 'r', encoding='latin1', errors='replace') as f:
                        original_text = f.read()
                    processed_text = self.preprocess_text(original_text)
                    self.samples.append({
                        'article': original_text,
                        'text': processed_text,
                        'category': category,
                        'label': self.label_map[category]
                    })

    def preprocess_text(self, text):
        """
        Preprocesses the input text:
        - Decodes from latin1 to utf-8, ignoring invalid characters.
        - Converts text to lowercase.
        - Removes stop words, named entities, non-alphabetic tokens, short tokens.
        - Applies lemmatization.
        - Removes punctuation and extra whitespace.

        Args:
            text (str): The original input text.

        Returns:
            str: Cleaned and lemmatized text.
        """
        # Fix encoding issues
        text = text.encode("latin1").decode("utf-8", "ignore")

        # Process the text using spaCy
        doc = nlp(text)
        tokens = []

        for token in doc:
            # Keep tokens that are not stop words, not named entities,
            # alphabetic, and longer than 1 character
            if not token.is_stop and token.is_alpha and not token.ent_type_ and len(token) > 1:
                tokens.append(token.lemma_)

        # Join lemmatized tokens
        cleaned_text = " ".join(tokens)

        # Remove punctuation
        cleaned_text = cleaned_text.translate(str.maketrans("", "", string.punctuation))

        # Remove excess whitespace
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

        return cleaned_text.lower()

    def __len__(self):
        """
        Returns the number of samples in the dataset.
        """
        return len(self.samples)

    def __getitem__(self, idx):
        """
        Returns a single sample from the dataset by index.

        Args:
            idx (int): Index of the sample.

        Returns:
            dict: A dictionary with the following keys:
                - 'article': original text
                - 'text': preprocessed text
                - 'category': category name (e.g., 'sport')
                - 'label': integer class label
        """
        return self.samples[idx]

    def __del__(self):
        """
        Cleans up the temporary extraction directory upon deletion.
        """
        if hasattr(self, "temp_dir"):
            self.temp_dir.cleanup()


dataset = Dataset_Loader("bbc.zip")
print(len(dataset))


pd.DataFrame([dataset.samples[0]])


dataset.categories








indices = list(range(len(dataset)))
train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)
train_dataset = Subset(dataset, train_idx)
test_dataset = Subset(dataset, test_idx)


X_train = [train_dataset[i]['text'] for i in range(len(train_dataset))]
y_train = [train_dataset[i]['category'] for i in range(len(train_dataset))]

X_test = [test_dataset[i]['text'] for i in range(len(test_dataset))]
y_test = [test_dataset[i]['category'] for i in range(len(test_dataset))]








# TF-IDF векторизация и SVM
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('svm', SVC(probability=True, class_weight='balanced'))
])


# Сетка гиперпараметров
param_grid = {
    # Подбор параметров для TF-IDF
    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],  # униграммы, биграммы, триграммы
    'tfidf__min_df': [1, 3, 4, 5],  # Минимальная частота слова

    # Подбор параметров для SVM
    'svm__C': [0.1, 1, 10],  # Коэффициент регуляризации
}


# GridSearchCV
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)


print('Best params:', grid_search.best_params_)
print('Best score:', grid_search.best_score_)


best_params = grid_search.best_params_
vect_params = {k.replace('tfidf__', ''): v for k, v in best_params.items() if k.startswith('tfidf__')}
svc_params = {k.replace('svc__', ''): v for k, v in best_params.items() if k.startswith('svc__')}








class SVMTextClassifier:
    """
    A text classification pipeline using SVM and TF-IDF vectorization.
    Supports training, evaluation, class prediction, and probability estimation.
    """

    def __init__(self, vectorizer_params=None, model_params=None):
        """
        Initialize the classifier.

        Args:
            vectorizer_params (dict): Parameters for TfidfVectorizer.
            model_params (dict): Parameters for sklearn SVC.
        """
        self.vectorizer = TfidfVectorizer(**(vectorizer_params or {}))
        self.model = SVC(**(model_params or {}), probability=True, class_weight='balanced')
        self.is_fitted = False

    def prepare_data(self, dataset: Dataset):
        """
        Extract text and label data from the dataset.

        Args:
            dataset (Dataset): Dataset returning a dict with 'text' and 'label'.

        Returns:
            tuple: (texts, labels)
        """
        texts = []
        labels = []
        for i in range(len(dataset)):
            sample = dataset[i]
            texts.append(sample['text'])
            labels.append(sample['label'])
        return texts, labels

    def plot_confusion_matrix(self, dataset: Dataset, label_names=None):
        """
        Plot the confusion matrix for predictions on the given dataset.
    
        Args:
            dataset (Dataset): Dataset with 'text' and 'label'.
            label_names (list of str): Class names for axis ticks.
        """
        texts, labels = self.prepare_data(dataset)
        preds = self.predict(dataset)
        self.cm = confusion_matrix(labels, preds)
    
        plt.figure(figsize=(8, 6))
        sns.heatmap(self.cm, annot=True, fmt="d", cmap="Blues",
                    xticklabels=label_names, yticklabels=label_names)
    
        plt.title("Confusion Matrix")
        plt.xlabel("Predicted Label")
        plt.ylabel("True Label")
        plt.tight_layout()
        plt.show()

    def train(self, dataset: Dataset):
        """
        Train the model on the given dataset.

        Args:
            dataset (Dataset): Dataset with 'text' and 'label'.
        """
        texts, labels = self.prepare_data(dataset)
        X = self.vectorizer.fit_transform(texts)
        self.model.fit(X, labels)
        self.is_fitted = True
        print("Model trained successfully.")

    def predict(self, dataset: Dataset):
        """
        Predict class labels for a Dataset.
    
        Args:
            dataset (Dataset): Dataset.
    
        Returns:
            list: Predicted class labels.
        """
        texts = [sample['text'] for sample in dataset]
        X = self.vectorizer.transform(texts)
        return self.model.predict(X)

    def predict_proba(self, dataset: Dataset):
        """
        Predict class probabilities for a Dataset.
    
        Args:
            dataset (Dataset): Dataset.
    
        Returns:
            ndarray: Array of shape (n_samples, n_classes) with class probabilities.
        """
        texts = [sample['text'] for sample in dataset]
        X = self.vectorizer.transform(texts)
        return self.model.predict_proba(X)

    def evaluate(self, dataset: Dataset):
        """
        Evaluate model accuracy and print classification report.
    
        Args:
            dataset (Dataset): Dataset with 'text' and 'label'.
    
        Returns:
            float: Accuracy score.
        """
        texts, labels = self.prepare_data(dataset)
        preds = self.predict(dataset)  
        acc = accuracy_score(labels, preds)
        print(f"Accuracy: {acc:.4f}")
        print("Classification Report:")
        print(classification_report(labels, preds, digits=4))
        return acc


clf = SVMTextClassifier(vectorizer_params=vect_params, model_params=svc_params)
clf.train(train_dataset)





clf.evaluate(test_dataset)





#Построим Confusion Matrix
clf.plot_confusion_matrix(test_dataset, label_names=dataset.categories)














class DistilBertTextClassifier:
    """
    A DistilBERT-based classifier for multiclass text classification.
    Uses raw article text from Dataset_Loader 
    """

    def __init__(self, model_name='distilbert-base-uncased', num_labels=5, lr=2e-5, batch_size=8, max_length=512):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)
        self.model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)
        self.model.to(self.device)

        self.lr = lr
        self.batch_size = batch_size
        self.max_length = max_length
        self.is_fitted = False

    def encode_dataset(self, dataset):
        """
        Tokenizes and encodes the dataset using the raw 'article' field.

        Args:
            dataset (Dataset): Dataset_Loader instance.

        Returns:
            tuple: (input_ids, attention_mask, labels) as torch tensors.
        """
        texts = [sample['article'] for sample in dataset]
        labels = [sample['label'] for sample in dataset]

        encodings = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=self.max_length,
            return_tensors='pt'
        )

        return encodings['input_ids'], encodings['attention_mask'], torch.tensor(labels)

    def plot_confusion_matrix(self, dataset, label_names=None, normalize=False):
        """
        Plot the confusion matrix for predictions on the given dataset.
    
        Args:
            dataset (Dataset): Dataset with 'article' and 'label'.
            label_names (list of str, optional): List of class names to display on axes.
            normalize (bool): If True, normalize the matrix row-wise.
        """
        true_labels = [sample['label'] for sample in dataset]
        preds = self.predict(dataset)
    
        self.cm = confusion_matrix(true_labels, preds)
    
    
        plt.figure(figsize=(8, 6))
        sns.heatmap(self.cm, annot=True, fmt="d", cmap="Blues",
                    xticklabels=label_names, yticklabels=label_names)
    
        plt.xlabel("Predicted Label")
        plt.ylabel("True Label")
        plt.title("Confusion Matrix")
        plt.tight_layout()
        plt.show()

    def train(self, dataset, epochs=3):
        """
        Fine-tunes the model on the given dataset using torch.optim.Adam instead of AdamW.
    
        Args:
            dataset (Dataset): Training dataset.
            epochs (int): Number of training epochs.
        """
        input_ids, attention_mask, labels = self.encode_dataset(dataset)
        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
    
        self.model.train()
        for epoch in range(epochs):
            print(f"\nEpoch {epoch + 1}/{epochs}")
            for start in range(0, len(labels), self.batch_size):
                end = start + self.batch_size
    
                input_batch = input_ids[start:end].to(self.device)
                mask_batch = attention_mask[start:end].to(self.device)
                label_batch = labels[start:end].to(self.device)
    
                outputs = self.model(input_ids=input_batch, attention_mask=mask_batch, labels=label_batch)
                loss = outputs.loss
                loss.backward()
    
                optimizer.step()
                optimizer.zero_grad()
    
                if not (start // self.batch_size) % 100:
                    print(f"Batch {start // self.batch_size + 1}: loss = {loss.item():.4f}")
    
        self.is_fitted = True
        print("Training complete.")

    def predict(self, dataset):
        """
        Predicts class labels for the given dataset.

        Args:
            dataset (Dataset): Dataset for predict.

        Returns:
            list: Predicted class labels.
        """
        input_ids, attention_mask, _ = self.encode_dataset(dataset)

        self.model.eval()
        with torch.no_grad():
            input_ids = input_ids.to(self.device)
            attention_mask = attention_mask.to(self.device)

            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=1).cpu().tolist()

        return preds

    def predict_proba(self, dataset):
        """
        Predicts class probabilities for the given dataset without using torch.nn.functional.
    
        Args:
            dataset (Dataset): Dataset for prediction.
    
        Returns:
            np.ndarray: Array of shape (n_samples, n_classes) with probabilities.
        """
        input_ids, attention_mask, _ = self.encode_dataset(dataset)
    
        self.model.eval()
        with torch.no_grad():
            input_ids = input_ids.to(self.device)
            attention_mask = attention_mask.to(self.device)
    
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits.cpu().numpy()
    
            # Apply softmax manually using numpy
            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # for numerical stability
            probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)
    
        return probs

    def evaluate(self, dataset):
        """
        Evaluates the model on the provided dataset.

        Args:
            dataset (Dataset): Dataset with ground-truth labels.

        Returns:
            float: Accuracy score.
        """
        true_labels = [sample['label'] for sample in dataset]
        preds = self.predict(dataset)

        acc = accuracy_score(true_labels, preds)
        print(f"\nAccuracy: {acc:.4f}")
        print("Classification Report:")
        print(classification_report(true_labels, preds, digits=4))
        return acc


clf2 = DistilBertTextClassifier(num_labels=5)
clf2.train(train_dataset, epochs=3)





clf2.evaluate(test_dataset)





clf2.plot_confusion_matrix(test_dataset, label_names=dataset.categories)











entertainment = [
    'https://www.bbc.com/news/articles/c4g007jjlyqo',
    'https://www.bbc.com/news/articles/cgkmmkyrl0vo',
    'https://www.bbc.com/news/articles/cqjd051z5ejo',
    'https://www.bbc.com/culture/article/20250227-10-of-the-best-films-to-watch-this-march',
    'https://www.bbc.com/culture/article/20250228-oscars-2025-who-will-win-and-who-should'
]


business = [
    'https://www.bbc.com/news/articles/cedll3282qzo',
    'https://www.bbc.com/news/articles/clydd7zeye7o', 
    'https://www.bbc.com/news/articles/ce8yy3wpn6eo',
    'https://www.bbc.com/news/articles/c4g7xn9y64po',
    'https://www.bbc.com/news/articles/ce980m2xv30o'
]


tech = [
    'https://www.bbc.com/news/articles/c5y0r8wdk62o',
    'https://www.bbc.com/news/articles/cn7vxlrvxyeo',
    'https://www.bbc.com/news/articles/cy877gydn48o',
    'https://www.bbc.com/future/article/20250228-5-youtube-videos-that-changed-the-way-we-think-about-ourselves',
    'https://www.bbc.com/news/articles/cgm18g19013o'
]


sport = [
    'https://www.bbc.com/sport/football/articles/c70w5q4kzkvo',
    'https://www.bbc.com/sport/football/articles/cjry7dj9v2po',
    'https://www.bbc.com/sport/football/articles/c9dej12zezno',
    'https://www.bbc.com/sport/athletics/articles/c3rn8qq2elxo',
    'https://www.bbc.com/sport/golf/articles/ce8vje3ldypo'
]


politics = [
    'https://www.bbc.com/news/articles/c7988r3q1p2o',
    'https://www.bbc.com/news/articles/cpv44982jlgo',
    'https://www.bbc.com/news/articles/cn9v1lzwqn7o',
    'https://www.bbc.com/news/articles/c981lr84013o',
    'https://www.bbc.com/news/articles/crmj298x7ypo'
]


bbc_val = {
    'entertainment': entertainment,
    'business': business,
    'tech': tech,
    'sport': sport,
    'politics': politics
}





Напишем метод для удобства, который сохранит новость по заданной ссылке.


def save_article_text(url, filename):
        """Fetches the main content and title of a news article from a given URL and saves them to a .txt file.
        Params:
            url: str
                Article url
            filename: str
                Path to save text
        """
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Находим и читаем заголовок
            title = soup.find('h1')
            title_text = title.get_text().strip() if title else ''
            
            # Читаем статью
            article_body = soup.find('article')
            if article_body:
                paragraphs = article_body.find_all('p')
                article_text = '\n'.join([p.get_text() for p in paragraphs])
                
                # Сохраняем
                with open(filename, 'w', encoding='utf-8') as file:
                    file.write(title_text)
                    file.write(article_text)
                print(f"Article saved successfully to {filename}")
            else:
                print("Fail.")
        else:
            print("Fail.")


# Сохраним в папку с аналогичной структурой с исходным данным
for key in bbc_val.keys():
    os.makedirs(f'validation/{key}', exist_ok=True)
    for i in range(0, len(bbc_val[key])):
        save_article_text( url = bbc_val[key][i], filename = f'validation/{key}/{i}.txt')





def zip_folder(folder_path, output_zip_path):
    """
    Create a ZIP archive from the contents of a folder.

    Args:
        folder_path (str): Path to the folder to zip.
        output_zip_path (str): Path to the output .zip file.
    """
    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, _, files in os.walk(folder_path):
            for file in files:
                file_path = os.path.join(root, file)
                # сохраняем относительный путь внутри архива
                arcname = os.path.relpath(file_path, start=folder_path)
                zipf.write(file_path, arcname)
    print(f"Folder '{folder_path}' zipped successfully to '{output_zip_path}'.")


zip_folder('validation', 'validation.zip')








dataset_new = Dataset_Loader('validation.zip')
print(len(dataset_new))





pd.DataFrame([dataset_new.samples[0]])


dataset_new.categories





preds = clf.predict(dataset_new)
probs = clf.predict_proba(dataset_new)


#Построим Confusion Matrix
clf.plot_confusion_matrix(dataset_new, label_names=dataset.categories)





# рассмотрим предсказания подробне
true_labels = [sample["label"] for sample in dataset_new]
label_names = dataset.categories
articles = [sample["article"] for sample in dataset_new]


# для удобства поместим все в pd.DataFrame
df = pd.DataFrame(probs, columns=[f"proba_{cls}" for cls in label_names])
df["true_label"] = [label_names[i] for i in true_labels]
df["predicted_label"] = [label_names[i] for i in preds]
df["article"] = articles  


df


Посмотрим на новсти, в которых модель ошиблась подробнее:


df.article[13]


df.article[20]











preds = clf2.predict(dataset_new)


probs = clf2.predict_proba(dataset_new)


clf2.plot_confusion_matrix(dataset_new,  label_names=dataset_new.categories)


# рассмотрим предсказания подробне
true_labels = [sample["label"] for sample in dataset_new]
label_names = dataset.categories
articles = [sample["article"] for sample in dataset_new]


# для удобства поместим все в pd.DataFrame
df = pd.DataFrame(probs, columns=[f"proba_{cls}" for cls in label_names])
df["true_label"] = [label_names[i] for i in true_labels]
df["predicted_label"] = [label_names[i] for i in preds]
df["article"] = articles  


df





df.article[9]


df.article[4]

















class SubsetByCategory(Dataset):
    def __init__(self, samples):
        self.samples = samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]


category_groups = {}
for sample in dataset:
    category = sample['category']
    if category not in category_groups:
        category_groups[category] = []
    category_groups[category].append(sample)


business_dataset = SubsetByCategory(category_groups['business'])
entertainment_dataset = SubsetByCategory(category_groups['entertainment'])
politics_dataset = SubsetByCategory(category_groups['politics'])
sport_dataset = SubsetByCategory(category_groups['sport'])
tech_dataset = SubsetByCategory(category_groups['tech'])








class LdaTopicModeling:
    """
    Perform LDA topic modeling on a dataset of preprocessed texts (from Dataset_Loader).
    """

    def __init__(self, dataset, passes=10):
        """
        Initialize the LDA model with preprocessed texts.

        Args:
            dataset (Dataset): Dataset_Loader instance.
            passes (int): Number of passes during training.
        """
        self.texts = [self.tokenize(sample['text']) for sample in dataset]
        self.passes = passes
        self.num_topics = None

        self.dictionary = corpora.Dictionary(self.texts)
        self.corpus = [self.dictionary.doc2bow(text) for text in self.texts]
        self.lda_model = None

    def tokenize(self, text):
        """
        Tokenize text and remove stopwords.

        Args:
            text (str): Preprocessed text.

        Returns:
            list[str]: Cleaned list of tokens.
        """
        tokens = word_tokenize(text.lower())
        return [t for t in tokens if t.isalpha() and t not in stopwords.words("english")]

    def find_optimal_num_topics(self, min_topics=2, max_topics=10, step=1, topn=10, plot=True):
        """
        Try different numbers of topics, compute coherence score for each, and optionally plot.
    
        Args:
            min_topics (int): Minimum number of topics to try.
            max_topics (int): Maximum number of topics to try.
            step (int): Step size between topic counts.
            topn (int): Number of top words per topic for coherence.
            plot (bool): Show a matplotlib plot of coherence scores.
    
        Returns:
            num_topics (int)
        """
        scores = []
        topic_range = range(min_topics, max_topics + 1, step)
    
        for num in topic_range:
            model = LdaModel(
                corpus=self.corpus,
                id2word=self.dictionary,
                num_topics=num,
                passes=self.passes,
                random_state=42
            )
            coherence_model = CoherenceModel(model=model, texts=self.texts, dictionary=self.dictionary, coherence='c_v', topn=topn)
            score = coherence_model.get_coherence()
            scores.append((num, score))
    
        # Plot if requested
        if plot:
            x, y = zip(*scores)
            plt.figure(figsize=(8, 5))
            plt.plot(x, y, marker='o', linestyle='-', color='steelblue')
            plt.xlabel("Number of Topics")
            plt.ylabel("Coherence Score (c_v)")
            plt.title("Optimal Number of Topics by Coherence")
            plt.grid(True)
            plt.xticks(x)
            plt.tight_layout()
            plt.show()
    
        # Best suggestion
        best = max(scores, key=lambda x: x[1])
        print(f"Best number of topics: {best[0]} (coherence = {best[1]:.4f})")
        self.num_topics = best[0]
        return best[0]

    def train(self):
        """
        Train the LDA model.
        """
        
        self.lda_model = LdaModel(
            corpus=self.corpus,
            id2word=self.dictionary,
            num_topics=self.num_topics,
            passes=self.passes,
            random_state=42
        )
        print("Training complete.")

    def print_topics(self, num_words=10):
        """
        Print top words in each topic.

        Args:
            num_words (int): Number of words to display per topic.
        """
        topics = self.lda_model.show_topics(num_topics=self.num_topics, num_words=num_words, formatted=False)
        for idx, topic in topics:
            keywords = [word for word, _ in topic]
            print(f"Topic {idx}: {', '.join(keywords)}")

    def plot_topic_keywords(self, topic_id, num_words=10):
        """
        Plot side-by-side bar chart and word cloud of keywords for a topic.
    
        Args:
            topic_id (int): ID of the topic.
            num_words (int): Number of top keywords to show.
        """
        # Get keywords
        topic_terms = self.lda_model.show_topic(topic_id, topn=num_words)
        words, weights = zip(*topic_terms)
    
        # subplots
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
        # Hist
        axes[0].barh(words[::-1], weights[::-1])
        axes[0].set_title(f"Topic {topic_id} — Top {num_words} Words (Weights)")
        axes[0].set_xlabel("Importance")
        axes[0].invert_yaxis()
    
        # Wordcloud
        word_freq = dict(topic_terms)
        wordcloud = WordCloud(width=400, height=300).generate_from_frequencies(word_freq)
    
        axes[1].imshow(wordcloud, interpolation='bilinear')
        axes[1].axis("off")
        axes[1].set_title(f"Topic {topic_id} — Word Cloud")
    
        plt.tight_layout()
        plt.show()

    def get_document_topics(self, idx):
        """
        Return topic distribution for a single document.

        Args:
            idx (int): Index of the document in the dataset.

        Returns:
            list[tuple[int, float]]: Topic ID and probability.
        """
        return self.lda_model.get_document_topics(self.corpus[idx])


def print_result(category_dataset):
    lda_model = LdaTopicModeling(category_dataset)
    num_topics = lda_model.find_optimal_num_topics(min_topics=2, max_topics=10, step=1, plot = True)
    lda_model.train()
    lda_model.print_topics(num_words=20)
    for i in range(0, num_topics):
        lda_model.plot_topic_keywords(topic_id=i, num_words=10)





print_result(business_dataset)








print_result(sport_dataset)








print_result(tech_dataset)








print_result(politics_dataset)








print_result(entertainment_dataset)















