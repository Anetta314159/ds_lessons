








import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
%matplotlib inline





class BaseDataProvider(object):
    """
    Base class for providing image data and corresponding labels for training segmentation models.

    This class is designed to be subclassed. Subclasses must implement the `_next_data` method
    that returns raw image data and the corresponding labels. The class handles normalization,
    label processing, and reshaping to fit the input requirements of a segmentation network.

    Attributes:
        channels (int): Number of input data channels (default is 1 for grayscale).
        n_class (int): Number of label classes (default is 2 for binary segmentation).
        a_min (float): Minimum value for clipping during normalization (default is -inf).
        a_max (float): Maximum value for clipping during normalization (default is +inf).

    Methods:
        __call__(n): Returns n samples of processed data and labels.
        _load_data_and_label(): Loads a single pair of processed data and labels.
        _process_data(data): Normalizes the input image data.
        _process_labels(label): Converts label into one-hot encoded format.
        _post_process(data, labels): Optional hook for post-processing (e.g., data augmentation).
    """
    channels = 1
    n_class = 2

    def __init__(self, a_min=None, a_max=None):
        """
        Initializes the data provider with optional normalization limits.
    
        Args:
            a_min (float, optional): Minimum value for clipping during normalization. Defaults to -inf.
            a_max (float, optional): Maximum value for clipping during normalization. Defaults to +inf.
        """
        self.a_min = a_min if a_min is not None else -np.inf
        self.a_max = a_max if a_min is not None else np.inf

    def _load_data_and_label(self):
        """
        Loads and processes a single data-label pair.
    
        Returns:
            tuple: A tuple (train_data, labels) where:
                - train_data (np.ndarray): Normalized input data of shape (1, H, W, channels).
                - labels (np.ndarray): One-hot encoded labels of shape (1, H, W, n_class).
        """
        data, label = self._next_data()

        train_data = self._process_data(data)
        labels = self._process_labels(label)

        train_data, labels = self._post_process(train_data, labels)

        nx = train_data.shape[1]
        ny = train_data.shape[0]

        return train_data.reshape(1, ny, nx, self.channels), labels.reshape(1, ny, nx, self.n_class),

    def _process_labels(self, label):
        """
        Converts binary labels into one-hot encoded format.
    
        If n_class == 2, assumes the input is a binary mask and creates a two-channel label
        where channel 0 is the background and channel 1 is the foreground.
    
        Args:
            label (np.ndarray): A 2D array of label data (should be boolean or convertible 
            to boolean).
    
        Returns:
            np.ndarray: One-hot encoded label array of shape (H, W, n_class).
        """
        if self.n_class == 2:
            nx = label.shape[1]
            ny = label.shape[0]
            labels = np.zeros((ny, nx, self.n_class), dtype=np.float32)

            # It is the responsibility of the child class to make sure that the label
            # is a boolean array, but we a chech here just in case.
            if label.dtype != 'bool':
                label = label.astype(bool)

            labels[..., 1] = label
            labels[..., 0] = ~label
            return labels

        return label

    def _process_data(self, data):
        """
        Normalizes the input data array by clipping, taking absolute values,
        and scaling to the range [0, 1].
    
        Args:
            data (np.ndarray): Input image data.
    
        Returns:
            np.ndarray: Normalized image data.
        """
        # normalization
        data = np.clip(np.fabs(data), self.a_min, self.a_max)
        data -= np.amin(data)

        if np.amax(data) != 0:
            data /= np.amax(data)

        return data

    def _post_process(self, data, labels):
        """
        Hook for additional processing or data augmentation. 
        Meant to be overridden by subclasses.
    
        Args:
            data (np.ndarray): Normalized data array.
            labels (np.ndarray): One-hot encoded label array.
    
        Returns:
            tuple: The possibly modified (data, labels) tuple.
        """
        return data, labels

    def __call__(self, n):
        """
        Generates a batch of n processed samples.
    
        Args:
            n (int): Number of samples to generate.
    
        Returns:
            tuple: A tuple (X, Y) where:
                - X (np.ndarray): Batch of input images of shape (n, H, W, channels).
                - Y (np.ndarray): Batch of label masks of shape (n, H, W, n_class).
        """
        train_data, labels = self._load_data_and_label()
        nx = train_data.shape[1]
        ny = train_data.shape[2]

        X = np.zeros((n, nx, ny, self.channels))
        Y = np.zeros((n, nx, ny, self.n_class))

        X[0] = train_data
        Y[0] = labels
        for i in range(1, n):
            train_data, labels = self._load_data_and_label()
            X[i] = train_data
            Y[i] = labels

        return X, Y


class GrayScaleDataProvider(BaseDataProvider):
    """
    Data provider for synthetic grayscale images with circle (and optional rectangle) masks.

    Inherits from BaseDataProvider and implements the _next_data method to generate
    images with corresponding segmentation labels. The image and label generation
    is controlled by the create_image_and_label function.

    Attributes:
        channels (int): Number of channels in the input image (1 for grayscale).
        n_class (int): Number of output classes (2 for circles only, 3 if rectangles included).
        nx (int): Width of the image.
        ny (int): Height of the image.
        kwargs (dict): Additional keyword arguments passed to the image generator.

    Args:
        nx (int): Width of the image.
        ny (int): Height of the image.
        **kwargs: Additional parameters passed to `create_image_and_label`.
                  If 'rectangles' is True, uses 3 output classes instead of 2.
    """
    # 1 for grayscale
    channels = 1
    # 2 for circles only
    n_class = 2

    def __init__(self, nx, ny, **kwargs):
        super(GrayScaleDataProvider, self).__init__()
        self.nx = nx
        self.ny = ny
        self.kwargs = kwargs
        rect = kwargs.get("rectangles", False)
        if rect:
            self.n_class=3

    def _next_data(self):
        """
        Generates the next synthetic image and corresponding label mask.
    
        Returns:
            tuple:
                - data (np.ndarray): A 2D grayscale image of shape (ny, nx).
                - label (np.ndarray): A 2D label mask with values representing different classes.
        """
        return create_image_and_label(self.nx, self.ny, **self.kwargs)


class RgbDataProvider(BaseDataProvider):
    """
    Data provider for synthetic RGB images with circle (and optional rectangle) masks.

    Extends BaseDataProvider and overrides _next_data to return 3-channel (RGB) images.
    Uses create_image_and_label to generate synthetic grayscale data, which is then converted
    to RGB using the to_rgb function.

    Attributes:
        channels (int): Number of channels in the input image (3 for RGB).
        n_class (int): Number of output classes (2 for circles only, 3 if rectangles included).
        nx (int): Width of the image.
        ny (int): Height of the image.
        kwargs (dict): Additional keyword arguments passed to the image generator.

    Args:
        nx (int): Width of the image.
        ny (int): Height of the image.
        **kwargs: Additional parameters passed to create_image_and_label.
                  If 'rectangles' is True, sets n_class = 3.
    """
    channels = 3
    n_class = 2

    def __init__(self, nx, ny, **kwargs):
        super(RgbDataProvider, self).__init__()
        self.nx = nx
        self.ny = ny
        self.kwargs = kwargs
        rect = kwargs.get("rectangles", False)
        if rect:
            self.n_class=3


    def _next_data(self):
        """
        Generates the next synthetic RGB image and corresponding label mask.
    
        Returns:
            tuple:
                - data (np.ndarray): A 3-channel RGB image of shape (ny, nx, 3).
                - label (np.ndarray): A 2D label mask with class values.
        """
        data, label = create_image_and_label(self.nx, self.ny, **self.kwargs)
        return to_rgb(data), label


def create_image_and_label(nx,ny, cnt = 10, r_min = 5, r_max = 50, border = 92, sigma = 20, rectangles=False):
    """
    Generates a synthetic grayscale image with circular and optionally rectangular objects,
    along with corresponding segmentation masks.

    The function creates an image of size (nx, ny) with cnt random circles (and optional
    rectangles), adds Gaussian noise, and returns the image along with its label mask(s).

    Args:
        nx (int): Width of the image.
        ny (int): Height of the image.
        cnt (int): Number of shapes (circles and/or rectangles) to generate. Default is 10.
        r_min (int): Minimum radius/size of shapes. Default is 5.
        r_max (int): Maximum radius/size of shapes. Default is 50.
        border (int): Minimum distance from the border to place shapes. Default is 92.
        sigma (float): Standard deviation of Gaussian noise added to the image. Default is 20.
        rectangles (bool): Whether to include rectangles in addition to circles. Default is False.

    Returns:
        tuple:
            - image (np.ndarray): A 2D grayscale image of shape (nx, ny, 1) with values in [0, 1].
            - label (np.ndarray):
                - If rectangles is False: a binary mask of shape (nx, ny) for circles only.
                - If rectangles is True: a one-hot encoded mask of shape (nx, ny, 3), where:
                    - channel 0: background
                    - channel 1: circles
                    - channel 2: rectangles
    """

    image = np.ones((nx, ny, 1))
    label = np.zeros((nx, ny, 3), dtype=bool)
    mask = np.zeros((nx, ny), dtype=bool)
    for _ in range(cnt):
        a = np.random.randint(border, nx-border)
        b = np.random.randint(border, ny-border)
        r = np.random.randint(r_min, r_max)
        h = np.random.randint(1,255)

        y,x = np.ogrid[-a:nx-a, -b:ny-b]
        m = x*x + y*y <= r*r
        mask = np.logical_or(mask, m)

        image[m] = h

    label[mask, 1] = 1

    if rectangles:
        mask = np.zeros((nx, ny), dtype=bool)
        for _ in range(cnt//2):
            a = np.random.randint(nx)
            b = np.random.randint(ny)
            r =  np.random.randint(r_min, r_max)
            h = np.random.randint(1,255)

            m = np.zeros((nx, ny), dtype=bool)
            m[a:a+r, b:b+r] = True
            mask = np.logical_or(mask, m)
            image[m] = h

        label[mask, 2] = 1

        label[..., 0] = ~(np.logical_or(label[...,1], label[...,2]))

    image += np.random.normal(scale=sigma, size=image.shape)
    image -= np.amin(image)
    image /= np.amax(image)

    if rectangles:
        return image, label
    else:
        return image, label[..., 1]


def to_rgb(img):
    """
    Converts a single-channel grayscale image into an RGB visualization using a custom colormap.

    This function applies a pseudo-color mapping based on pixel intensity to enhance visualization.
    It normalizes the input, handles NaN values, and maps intensity to red, green, and blue channels.

    Args:
        img (np.ndarray): A 2D or 3D grayscale image array of shape (H, W) or (H, W, 1).

    Returns:
        np.ndarray: An RGB image of shape (H, W, 3), with values in the range [0, 1].
    """
    img = img.reshape(img.shape[0], img.shape[1])
    img[np.isnan(img)] = 0
    img -= np.amin(img)
    img /= np.amax(img)
    blue = np.clip(4*(0.75-img), 0, 1)
    red  = np.clip(4*(img-0.25), 0, 1)
    green= np.clip(44*np.fabs(img-0.5)-1., 0, 1)
    rgb = np.stack((red, green, blue), axis=2)
    return rgb


nx = 572
ny = 572
generator = GrayScaleDataProvider(nx, ny, cnt=20)
x_test, y_test = generator(1)

fig, ax = plt.subplots(1, 2, sharey=True, figsize=(8, 4))
ax[0].imshow(x_test[0, ..., 0], aspect="auto")
ax[1].imshow(y_test[0, ..., 1], aspect="auto")

plt.show()


import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision import transforms
from sklearn.metrics import jaccard_score
import random
import cv2








class CircleSegmentationDataset(Dataset):
    """
    PyTorch Dataset for generating synthetic grayscale images with circular segmentation masks.

    This dataset wraps around a data provider GrayScaleDataProvider, that generates
    synthetic data on-the-fly. Each sample returned is a tensor pair (image, mask), suitable
    for training segmentation models.

    Attributes:
        provider (BaseDataProvider): A data provider instance that generates images and masks.
        count (int): Total number of samples in the dataset.

    Args:
        provider (BaseDataProvider): Instance of a data provider GrayScaleDataProvider.
        count (int): Number of samples to be generated in the dataset.
    """
    def __init__(self, provider, count):
        self.provider = provider
        self.count = count

    def __len__(self):
        """
        Returns the total number of samples in the dataset.
    
        Returns:
            int: The number of samples.
        """
        return self.count

    def __getitem__(self, idx):
        """
        Generates and returns a single sample from the dataset.
    
        Args:
            idx (int): Index of the sample (not used since data is generated on-the-fly).
    
        Returns:
            tuple:
                - image (torch.Tensor): Normalized grayscale image tensor of shape (1, H, W).
                - mask (torch.Tensor): Binary mask tensor of shape (H, W), corresponding to the object class.
        """
        x, y = self.provider(1)
        x = x[0].transpose(2, 0, 1)  # to (C, H, W)
        y = y[0].transpose(2, 0, 1)  # to (n_class, H, W)
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y[1], dtype=torch.float32)











class UNet(nn.Module):
    """
    A simplified UNet architecture for binary image segmentation.

    The network follows an encoderâ€“bottleneckâ€“decoder structure with skip connections.
    It uses two levels of downsampling and upsampling, and outputs a single-channel
    prediction map with values in [0, 1], representing the probability of the foreground class.

    Architecture:
        - Encoder: 2 downsampling blocks (Conv + ReLU + MaxPool)
        - Bottleneck: double conv block
        - Decoder: 2 upsampling blocks with skip connections
        - Output: 1x1 convolution followed by sigmoid activation for binary segmentation

    Input shape:
        (B, 1, H, W) â€” grayscale image batch

    Output shape:
        (B, 1, H, W) â€” segmentation probability map
    """
    def __init__(self):
        super(UNet, self).__init__()
        def CBR(in_ch, out_ch):
            return nn.Sequential(
                nn.Conv2d(in_ch, out_ch, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_ch, out_ch, 3, padding=1),
                nn.ReLU(inplace=True),
            )
        self.enc1 = CBR(1, 64)
        self.pool1 = nn.MaxPool2d(2)
        self.enc2 = CBR(64, 128)
        self.pool2 = nn.MaxPool2d(2)
        self.bottleneck = CBR(128, 256)
        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = CBR(256, 128)
        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = CBR(128, 64)
        self.final = nn.Conv2d(64, 1, 1)

    def forward(self, x):
        """
        Forward pass of the UNet model.
    
        Args:
            x (torch.Tensor): Input tensor of shape (B, 1, H, W), where B is batch size.
    
        Returns:
            torch.Tensor: Output tensor of shape (B, 1, H, W) with sigmoid activation,
                          representing per-pixel probabilities for the foreground class.
        """
        # ÐŸÑ€Ð¾Ñ…Ð¾Ð´ Ñ‡ÐµÑ€ÐµÐ· Ð¿ÐµÑ€Ð²Ñ‹Ð¹ ÑÐ½ÐºÐ¾Ð´ÐµÑ€Ð½Ñ‹Ð¹ Ð±Ð»Ð¾Ðº (Ð´Ð²Ðµ ÑÐ²ÐµÑ€Ñ‚ÐºÐ¸ Ñ ReLU Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸ÐµÐ¹).
        # Ð’Ñ…Ð¾Ð´ x Ð¸Ð¼ÐµÐµÑ‚ Ñ„Ð¾Ñ€Ð¼Ñƒ (B, 1, H, W) â€” Ð¾Ð´Ð¸Ð½ ÐºÐ°Ð½Ð°Ð» Ð´Ð»Ñ Ð±Ð¸Ð½Ð°Ñ€Ð½Ð¾Ð¹ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸.
        # Ð’Ñ‹Ñ…Ð¾Ð´ e1 Ð±ÑƒÐ´ÐµÑ‚ Ð¸Ð¼ÐµÑ‚ÑŒ Ñ„Ð¾Ñ€Ð¼Ñƒ (B, 64, H, W), Ñ‚Ð¾ ÐµÑÑ‚ÑŒ 64 ÐºÐ°Ð½Ð°Ð»Ð° Ð¿Ð¾ÑÐ»Ðµ ÑÐ²ÐµÑ€Ñ‚Ð¾Ðº.
        e1 = self.enc1(x)
        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ (MaxPool2d) Ñ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð¾Ð¼ Ð¾ÐºÐ½Ð° 2x2 Ð´Ð»Ñ ÑƒÐ¼ÐµÐ½ÑŒÑˆÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ñ (H, W).
        # Ð”Ð°Ð»ÐµÐµ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´Ð¸Ð¼ Ñ‡ÐµÑ€ÐµÐ· Ð²Ñ‚Ð¾Ñ€Ð¾Ð¹ ÑÐ½ÐºÐ¾Ð´ÐµÑ€Ð½Ñ‹Ð¹ Ð±Ð»Ð¾Ðº, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ‚Ð°ÐºÐ¶Ðµ ÑÐ¾ÑÑ‚Ð¾Ð¸Ñ‚ Ð¸Ð· Ð´Ð²ÑƒÑ… ÑÐ²ÐµÑ€Ñ‚Ð¾Ðº Ñ ReLU Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸ÐµÐ¹.
        # Ð’ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ðµ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ e2 Ñ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒÑŽ (B, 128, H/2, W/2).
        e2 = self.enc2(self.pool1(e1))
        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ ÐµÑ‰Ðµ Ð¾Ð´Ð½Ð¾ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ ÑÐ¶Ð°Ñ‚Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² bottleneck Ð±Ð»Ð¾Ðº.
        # Ð‘Ð»Ð¾Ðº bottleneck ÑÐ¾ÑÑ‚Ð¾Ð¸Ñ‚ Ð¸Ð· Ð´Ð²ÑƒÑ… ÑÐ²ÐµÑ€Ñ‚Ð¾Ðº Ñ ReLU Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸ÐµÐ¹ Ð¸ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð±Ð¾Ð»ÐµÐµ Ð°Ð±ÑÑ‚Ñ€Ð°ÐºÑ‚Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸.
        # Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ b Ð±ÑƒÐ´ÐµÑ‚ Ð¸Ð¼ÐµÑ‚ÑŒ Ñ„Ð¾Ñ€Ð¼Ñƒ (B, 256, H/4, W/4), Ñ‚Ð°Ðº ÐºÐ°Ðº pooling ÑƒÐ¼ÐµÐ½ÑŒÑˆÐ°ÐµÑ‚ Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð½Ð° 2x.
        b = self.bottleneck(self.pool2(e2))
        # ÐŸÑ€Ð¾Ñ…Ð¾Ð´Ð¸Ð¼ Ñ‡ÐµÑ€ÐµÐ· Ð´ÐµÐºÐ¾Ð´ÐµÑ€Ð½Ñ‹Ð¹ Ð±Ð»Ð¾Ðº 2 (dec2), ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Ñ‚Ñ€Ð°Ð½ÑÐ¿Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ ÑÐ²ÐµÑ€Ñ‚ÐºÑƒ (up2).
        # Ð­Ñ‚Ð¾ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ñ Ð°Ð¿ÑÐµÐ¼Ð¿Ð»Ð¸Ð½Ð³Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÑ‚ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ, ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ð² Ð²Ñ‹ÑÐ¾Ñ‚Ñƒ Ð¸ ÑˆÐ¸Ñ€Ð¸Ð½Ñƒ.
        # Ð—Ð°Ñ‚ÐµÐ¼ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ ÐºÐ¾Ð½ÐºÐ°Ñ‚ÐµÐ½Ð°Ñ†Ð¸ÑŽ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð° Ð°Ð¿ÑÐµÐ¼Ð¿Ð»Ð¸Ð½Ð³Ð° (up2) Ñ Ð²Ñ‹Ñ…Ð¾Ð´Ð¾Ð¼ Ð¸Ð· Ð²Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ ÑÐ½ÐºÐ¾Ð´ÐµÑ€Ð° (e2) Ð²Ð´Ð¾Ð»ÑŒ ÐºÐ°Ð½Ð°Ð»Ð° (dim=1).
        # Ð¢Ð°ÐºÐ¸Ð¼ Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð¼, Ð¼Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ skip-ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‚ÑŒ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ñ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ñ… ÑÐ»Ð¾ÐµÐ² Ð² Ð´ÐµÐºÐ¾Ð´ÐµÑ€.
        # Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ d2 Ð¸Ð¼ÐµÐµÑ‚ Ñ„Ð¾Ñ€Ð¼Ñƒ (B, 128, H/2, W/2).
        d2 = self.dec2(torch.cat([self.up2(b), e2], dim=1))
        # ÐŸÑ€Ð¾Ñ†ÐµÑÑ Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸Ñ‡ÐµÐ½ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÐµÐ¼Ñƒ: Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ñ‚Ñ€Ð°Ð½ÑÐ¿Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ ÑÐ²ÐµÑ€Ñ‚ÐºÑƒ (up1) Ð´Ð»Ñ Ð°Ð¿ÑÐµÐ¼Ð¿Ð»Ð¸Ð½Ð³Ð°.
        # Ð”Ð°Ð»ÐµÐµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ ÐºÐ¾Ð½ÐºÐ°Ñ‚ÐµÐ½Ð°Ñ†Ð¸ÑŽ Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð¼ Ð¸Ð· Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ ÑÐ½ÐºÐ¾Ð´ÐµÑ€Ð° (e1), Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· Ð±Ð¾Ð»ÐµÐµ Ñ€Ð°Ð½Ð½Ð¸Ñ… ÑÐ»Ð¾ÐµÐ².
        # Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ d1 Ð¸Ð¼ÐµÐµÑ‚ Ñ„Ð¾Ñ€Ð¼Ñƒ (B, 64, H, W), Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°Ñ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ð¾Ðµ Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ.
        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))
        # Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ»Ð¾Ð¹ â€” ÑÑ‚Ð¾ ÑÐ²ÐµÑ€Ñ‚Ð¾Ñ‡Ð½Ñ‹Ð¹ ÑÐ»Ð¾Ð¹ Ñ ÑÐ´Ñ€Ð¾Ð¼ 1x1, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑÐ¶Ð¸Ð¼Ð°ÐµÑ‚ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ°Ð½Ð°Ð»Ð¾Ð²
        # Ñ 64 Ð´Ð¾ 1 (Ñ‚.Ðµ. Ð¾Ð´Ð¸Ð½ ÐºÐ°Ð½Ð°Ð» Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸ Ðº ÐºÐ»Ð°ÑÑÑƒ ÐºÑ€ÑƒÐ¶Ð¾Ðº).
        # ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ â€” ÑÑ‚Ð¾ Ð»Ð¾Ð³Ð¸Ñ‚ (Ð½ÐµÐ¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ), Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ Ð¼Ñ‹ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼
        # ÑÐ¸Ð³Ð¼Ð¾Ð¸Ð´Ñƒ torch.sigmoid(), Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ñ€Ð¸Ð²ÐµÑÑ‚Ð¸ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ðº Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ñƒ [0, 1].
        
        # Ð¢Ð°ÐºÐ¸Ð¼ Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð¼, Ð½Ð° Ð²Ñ‹Ñ…Ð¾Ð´Ðµ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÐºÐ°Ñ€Ñ‚Ñƒ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÐµÐ¹:
        # - ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð¿Ð¸ÐºÑÐµÐ»ÑŒ Ð¸Ð¼ÐµÐµÑ‚ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¾Ñ‚ 0 Ð´Ð¾ 1,
        # - Ð³Ð´Ðµ 0 Ð¾Ð·Ð½Ð°Ñ‡Ð°ÐµÑ‚ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ Ð² "Ñ„Ð¾Ð½Ðµ", Ð° 1 â€” ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ Ð² "Ð¾Ð±ÑŠÐµÐºÑ‚Ðµ".
        
        # Ð­Ñ‚Ð¾ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð¿Ñ€Ð¸ Ð±Ð¸Ð½Ð°Ñ€Ð½Ð¾Ð¹ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸, Ð³Ð´Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸
        # ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ð¸ÐºÑÐµÐ»Ñ Ðº Ð¾Ð´Ð½Ð¾Ð¼Ñƒ ÐºÐ»Ð°ÑÑÑƒ ("ÐºÑ€ÑƒÐ¶Ð¾Ðº" Ð¸Ð»Ð¸ "Ð½Ðµ ÐºÑ€ÑƒÐ¶Ð¾Ðº").
        return torch.sigmoid(self.final(d1))








# Ð Ð°Ð·Ð¼ÐµÑ€Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð²Ð°ÐµÐ¼Ð¾Ð³Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ (ÑˆÐ¸Ñ€Ð¸Ð½Ð° Ð¸ Ð²Ñ‹ÑÐ¾Ñ‚Ð°)
nx = 572
ny = 572

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€ Ð´Ð°Ð½Ð½Ñ‹Ñ…, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ 20 ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ð¼Ð¸ ÐºÑ€ÑƒÐ³Ð°Ð¼Ð¸
provider = GrayScaleDataProvider(nx, ny, cnt=20)
# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ PyTorch Dataset, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð±ÑƒÐ´ÐµÑ‚ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÑ‚ÑŒ 100 ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸ Ð¼Ð°ÑÐ¾Ðº
dataset = CircleSegmentationDataset(provider, count=100)





# Ð Ð°Ð·Ð¼ÐµÑ€ Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰ÐµÐ¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸ â€” 60% 
train_size = int(0.6 * len(dataset))
# Ð Ð°Ð·Ð¼ÐµÑ€ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸ â€” 20% 
val_size = int(0.2 * len(dataset))
test_size = len(dataset) - train_size - val_size
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])





batch_size = 5
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)








device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = UNet().to(device)
# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ â€” Ð±Ð¸Ð½Ð°Ñ€Ð½Ð°Ñ ÐºÑ€Ð¾ÑÑ-ÑÐ½Ñ‚Ñ€Ð¾Ð¿Ð¸Ñ.
criterion = nn.BCELoss()
# ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ â€” Adam
optimizer = optim.Adam(model.parameters(), lr=0.001)





# Ð¡Ð¿Ð¸ÑÐºÐ¸ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ
train_losses, val_losses = [], []
# ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ¿Ð¾Ñ… 
epochs = 10

for epoch in range(epochs):
    model.train()
    total_train_loss = 0
    for x, y in train_loader:
        x, y = x.to(device), y.to(device)
        # ÐžÐ±Ð½ÑƒÐ»ÑÐµÐ¼ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ñ‹ Ð¿ÐµÑ€ÐµÐ´ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ñ‹Ð¼ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´Ð¾Ð¼
        optimizer.zero_grad()
        # ÐŸÑ€ÑÐ¼Ð¾Ð¹ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´ â€” Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð¾Ñ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        out = model(x)
        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ
        loss = criterion(out, y.unsqueeze(1))
        # ÐžÐ±Ñ€Ð°Ñ‚Ð½Ð¾Ðµ Ñ€Ð°ÑÐ¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ (backpropagation)
        loss.backward()
        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½Ð¾Ð³Ð¾ ÑÐ¿ÑƒÑÐºÐ° (Adam)
        optimizer.step()
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ Ð´Ð»Ñ ÑÑ‚Ð¾Ð¹ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸
        total_train_loss += loss.item()
    # Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð·Ð° Ð²ÑÑŽ Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰ÑƒÑŽ ÑÐ¿Ð¾Ñ…Ñƒ
    train_losses.append(total_train_loss / len(train_loader))

    model.eval()
    #Valid loss
    total_test_loss = 0
    with torch.no_grad():
        for x, y in val_loader:
            x, y = x.to(device), y.to(device)
            # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð¾Ñ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸
            out = model(x)
            # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ Ð¿Ð¾Ñ‚ÐµÑ€ÑŒ
            loss = criterion(out, y.unsqueeze(1))
            total_test_loss += loss.item()
    # Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð½Ð° Ñ‚ÐµÑÑ‚Ðµ Ð·Ð° Ñ‚ÐµÐºÑƒÑ‰ÑƒÑŽ ÑÐ¿Ð¾Ñ…Ñƒ
    val_losses.append(total_test_loss / len(val_loader))
    
    print(f"Epoch {epoch+1}/{epochs} - "
          f"Train Loss: {train_losses[-1]:.4f}, "
          f"Val Loss: {val_losses[-1]:.4f}")








plt.figure(figsize=(8, 4))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')  
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training and Test Loss")
plt.legend()
plt.grid(True)
plt.show()











# Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð´Ð»Ñ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ IoU (Intersection over Union) Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ
ious = []
# ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð² 
with torch.no_grad():
    for x, y in val_loader:
        x, y = x.to(device), y.to(device)
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð¾Ñ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸ 
        out = model(x)
        # ÐŸÑ€ÐµÐ²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ð² Ð±Ð¸Ð½Ð°Ñ€Ð½Ñ‹Ðµ Ð¼Ð°ÑÐºÐ¸
        pred = (out > 0.5).float()
        for p, t in zip(pred, y.unsqueeze(1)):
            # ÐŸÐµÑ€ÐµÐ±Ð¸Ñ€Ð°ÐµÐ¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² Ð±Ð°Ñ‚Ñ‡Ðµ
            # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ IoU
            iou = jaccard_score(t.cpu().numpy().flatten(), p.cpu().numpy().flatten())
            ious.append(iou)
# Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ ÑÑ€ÐµÐ´Ð½ÐµÐµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ IoU
print(f"Mean IoU on validation set: {np.mean(ious):.4f}")








example_batch = next(iter(val_loader))
images, true_masks = example_batch
images, true_masks = images.to(device), true_masks.to(device)

with torch.no_grad():
    predicted_masks = model(images)
    predicted_masks = (predicted_masks > 0.5).float()

# Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð²
num_samples = min(4, images.shape[0])
fig, axs = plt.subplots(num_samples, 3, figsize=(10, 3 * num_samples))

for i in range(num_samples):
    axs[i, 0].imshow(images[i][0].cpu(), cmap='gray')
    axs[i, 0].set_title("Input Image")
    axs[i, 1].imshow(true_masks[i].cpu(), cmap='gray')
    axs[i, 1].set_title("Ground Truth Mask")
    axs[i, 2].imshow(predicted_masks[i][0].cpu(), cmap='gray')
    axs[i, 2].set_title("Predicted Mask")
    for j in range(3):
        axs[i, j].axis('off')

plt.tight_layout()
plt.show()








from sklearn.metrics import jaccard_score, f1_score, accuracy_score, precision_score, recall_score


ious = []
dices = []
accuracies = []
precisions = []
recalls = []
f1s = []

model.eval()
with torch.no_grad():
    for x, y in test_loader:
        x, y = x.to(device), y.to(device)
        out = model(x)
        pred = (out > 0.5).float()

        for p, t in zip(pred, y.unsqueeze(1)):
            y_true = t.cpu().numpy().flatten()
            y_pred = p.cpu().numpy().flatten()
        
            ious.append(jaccard_score(y_true, y_pred))
            dices.append(2 * np.logical_and(y_true, y_pred).sum() / (y_true.sum() + y_pred.sum() + 1e-8))
            accuracies.append(accuracy_score(y_true, y_pred))
            precisions.append(precision_score(y_true, y_pred))
            recalls.append(recall_score(y_true, y_pred))
            f1s.append(f1_score(y_true, y_pred))

# ðŸ“Š Ð’Ñ‹Ð²Ð¾Ð´ ÑÑ€ÐµÐ´Ð½Ð¸Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ Ð²ÑÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÐ¼
print("Test metrics:")
print(f"Mean IoU:       {np.mean(ious):.4f}")
print(f"Mean Dice:      {np.mean(dices):.4f}")
print(f"Accuracy:       {np.mean(accuracies):.4f}")
print(f"Precision:      {np.mean(precisions):.4f}")
print(f"Recall:         {np.mean(recalls):.4f}")
print(f"F1 Score:       {np.mean(f1s):.4f}")








n = 5  
examples = 0

with torch.no_grad():
    for x, y in test_loader:
        x, y = x.to(device), y.to(device)
        out = model(x)
        pred = (out > 0.5).float()

        for i in range(x.size(0)):
            img = x[i, 0].cpu().numpy()                    # (H, W), ÐºÐ°Ð½Ð°Ð» Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ
            true_mask = y[i].cpu().numpy()                # (H, W), ground truth Ð¼Ð°ÑÐºÐ°
            pred_mask = pred[i, 0].cpu().numpy()          # (H, W), predicted Ð¼Ð°ÑÐºÐ°

            # ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð¶Ð°ÐµÐ¼
            plt.figure(figsize=(12, 4))
            
            plt.subplot(1, 3, 1)
            plt.imshow(img, cmap='gray')
            plt.title("Input Image")
            plt.axis("off")

            plt.subplot(1, 3, 2)
            plt.imshow(true_mask, cmap='gray')
            plt.title("Ground Truth")
            plt.axis("off")

            plt.subplot(1, 3, 3)
            plt.imshow(pred_mask, cmap='gray')
            plt.title("Prediction")
            plt.axis("off")

            plt.tight_layout()
            plt.show()

            examples += 1
            if examples >= n:
                break
        if examples >= n:
            break



